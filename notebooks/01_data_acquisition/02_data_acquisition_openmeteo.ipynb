{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Acquisition: Open-Meteo Historical Weather API\n",
        "\n",
        "This notebook downloads historical weather data from Open-Meteo API for rapid prototyping and pipeline validation.\n",
        "\n",
        "## Objectives\n",
        "1. Download historical weather data for selected stations\n",
        "2. Validate data pipeline before scaling to NOAA ISD\n",
        "3. Test data loading and preprocessing functions\n",
        "4. Create sample dataset for development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\Users\\Kata\\Desktop\\earth-sgnn\n",
            "Raw data directory: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\n",
            "Processed data directory: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Setup project path - this adds project root to Python path\n",
        "# First, we need to temporarily add a path to import the utility\n",
        "# We'll use a simple approach that works from any notebook location\n",
        "current_dir = Path(os.getcwd()).resolve()\n",
        "\n",
        "# Find project root by looking for src/ and notebooks/ directories\n",
        "if (current_dir / 'src').exists() and (current_dir / 'notebooks').exists():\n",
        "    project_root = current_dir\n",
        "elif current_dir.name in ['01_data_acquisition', '02_data_preprocessing', '03_baselines', \n",
        "                           '04_gnn_models', '05_training', '06_analysis', '07_evaluation', '08_documentation']:\n",
        "    project_root = current_dir.parent.parent\n",
        "elif current_dir.name == 'notebooks':\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    # Walk up to find project root\n",
        "    for parent in current_dir.parents:\n",
        "        if (parent / 'src').exists() and (parent / 'notebooks').exists():\n",
        "            project_root = parent\n",
        "            break\n",
        "    else:\n",
        "        project_root = current_dir\n",
        "\n",
        "# Add project root to Python path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Now we can import from src\n",
        "from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
        "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define Station Locations\n",
        "\n",
        "We'll start with a small set of stations for prototyping. These are major cities with good data coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of stations: 10\n",
            "\n",
            "Station locations:\n",
            "  New York: (40.7128, -74.0060), elevation: 10m\n",
            "  Los Angeles: (34.0522, -118.2437), elevation: 71m\n",
            "  Chicago: (41.8781, -87.6298), elevation: 182m\n",
            "  Houston: (29.7604, -95.3698), elevation: 13m\n",
            "  Phoenix: (33.4484, -112.0740), elevation: 331m\n",
            "  Philadelphia: (39.9526, -75.1652), elevation: 12m\n",
            "  San Antonio: (29.4241, -98.4936), elevation: 198m\n",
            "  San Diego: (32.7157, -117.1611), elevation: 19m\n",
            "  Dallas: (32.7767, -96.7970), elevation: 131m\n",
            "  San Jose: (37.3382, -121.8863), elevation: 26m\n"
          ]
        }
      ],
      "source": [
        "# Sample stations for prototyping (major cities)\n",
        "STATIONS = {\n",
        "    'New York': {'lat': 40.7128, 'lon': -74.0060, 'elevation': 10},\n",
        "    'Los Angeles': {'lat': 34.0522, 'lon': -118.2437, 'elevation': 71},\n",
        "    'Chicago': {'lat': 41.8781, 'lon': -87.6298, 'elevation': 182},\n",
        "    'Houston': {'lat': 29.7604, 'lon': -95.3698, 'elevation': 13},\n",
        "    'Phoenix': {'lat': 33.4484, 'lon': -112.0740, 'elevation': 331},\n",
        "    'Philadelphia': {'lat': 39.9526, 'lon': -75.1652, 'elevation': 12},\n",
        "    'San Antonio': {'lat': 29.4241, 'lon': -98.4936, 'elevation': 198},\n",
        "    'San Diego': {'lat': 32.7157, 'lon': -117.1611, 'elevation': 19},\n",
        "    'Dallas': {'lat': 32.7767, 'lon': -96.7970, 'elevation': 131},\n",
        "    'San Jose': {'lat': 37.3382, 'lon': -121.8863, 'elevation': 26}\n",
        "}\n",
        "\n",
        "print(f\"Number of stations: {len(STATIONS)}\")\n",
        "print(\"\\nStation locations:\")\n",
        "for name, info in STATIONS.items():\n",
        "    print(f\"  {name}: ({info['lat']:.4f}, {info['lon']:.4f}), elevation: {info['elevation']}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Date Range\n",
        "\n",
        "We'll download data for the past 2 years for prototyping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start date: 2024-02-05\n",
            "End date: 2026-02-04\n",
            "Total days: 730\n"
          ]
        }
      ],
      "source": [
        "# Date range for data download\n",
        "# Note: Open-Meteo archive API typically has data up to a few days ago\n",
        "# We'll use yesterday as end date to avoid future date issues\n",
        "END_DATE = datetime.now() - timedelta(days=1)  # Yesterday (to ensure data is available)\n",
        "START_DATE = END_DATE - timedelta(days=730)  # 2 years of data\n",
        "\n",
        "print(f\"Start date: {START_DATE.strftime('%Y-%m-%d')}\")\n",
        "print(f\"End date: {END_DATE.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Total days: {(END_DATE - START_DATE).days}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Open-Meteo API Function\n",
        "\n",
        "Function to download historical weather data from Open-Meteo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_openmeteo_data(latitude, longitude, start_date, end_date, station_name=None):\n",
        "    \"\"\"\n",
        "    Download historical weather data from Open-Meteo API.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    latitude : float\n",
        "        Latitude of the location\n",
        "    longitude : float\n",
        "        Longitude of the location\n",
        "    start_date : datetime\n",
        "        Start date for data\n",
        "    end_date : datetime\n",
        "        End date for data\n",
        "    station_name : str, optional\n",
        "        Name of the station (for logging)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with weather data\n",
        "    \"\"\"\n",
        "    \n",
        "    # Open-Meteo API endpoint\n",
        "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "    \n",
        "    # Parameters\n",
        "    # Note: hourly must be a comma-separated string, not a list\n",
        "    hourly_variables = [\n",
        "        \"temperature_2m\",\n",
        "        \"relative_humidity_2m\",\n",
        "        \"dewpoint_2m\",\n",
        "        \"wind_speed_10m\",\n",
        "        \"wind_direction_10m\",\n",
        "        \"surface_pressure\"\n",
        "    ]\n",
        "    \n",
        "    params = {\n",
        "        \"latitude\": latitude,\n",
        "        \"longitude\": longitude,\n",
        "        \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
        "        \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
        "        \"hourly\": \",\".join(hourly_variables),  # Join list into comma-separated string\n",
        "        \"timezone\": \"UTC\"\n",
        "    }\n",
        "    \n",
        "    response = None\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        \n",
        "        # Check if hourly data exists\n",
        "        if 'hourly' not in data:\n",
        "            print(f\"Warning: No hourly data in response for {station_name or f'({latitude}, {longitude})'}\")\n",
        "            return None\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(data['hourly'])\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"Warning: Empty data for {station_name or f'({latitude}, {longitude})'}\")\n",
        "            return None\n",
        "        \n",
        "        # Convert time to datetime\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        \n",
        "        # Add station metadata\n",
        "        df['latitude'] = latitude\n",
        "        df['longitude'] = longitude\n",
        "        if station_name:\n",
        "            df['station_name'] = station_name\n",
        "        \n",
        "        # Rename columns for consistency\n",
        "        column_mapping = {\n",
        "            'time': 'timestamp',\n",
        "            'temperature_2m': 'temperature_2m',\n",
        "            'relative_humidity_2m': 'relative_humidity_2m',\n",
        "            'dewpoint_2m': 'dewpoint_2m',\n",
        "            'wind_speed_10m': 'wind_speed_10m',\n",
        "            'wind_direction_10m': 'wind_direction_10m',\n",
        "            'surface_pressure': 'surface_pressure'\n",
        "        }\n",
        "        df = df.rename(columns=column_mapping)\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # Try to get error details from response\n",
        "        error_msg = str(e)\n",
        "        response_url = \"\"\n",
        "        try:\n",
        "            if e.response is not None:\n",
        "                if hasattr(e.response, 'json'):\n",
        "                    try:\n",
        "                        error_data = e.response.json()\n",
        "                        error_msg = error_data.get('reason', error_data.get('error', str(e)))\n",
        "                    except:\n",
        "                        pass\n",
        "                if hasattr(e.response, 'url'):\n",
        "                    response_url = e.response.url\n",
        "        except:\n",
        "            pass\n",
        "        print(f\"HTTP Error for {station_name or f'({latitude}, {longitude})'}: {error_msg}\")\n",
        "        if response_url:\n",
        "            print(f\"  URL: {response_url}\")\n",
        "        elif response and hasattr(response, 'url'):\n",
        "            print(f\"  URL: {response.url}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {station_name or f'({latitude}, {longitude})'}: {e}\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error parsing response for {station_name or f'({latitude}, {longitude})'}: {e}\")\n",
        "        print(f\"  Response keys: {list(data.keys()) if 'data' in locals() else 'N/A'}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error for {station_name or f'({latitude}, {longitude})'}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Download Data for All Stations\n",
        "\n",
        "Download data for all stations with progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from Open-Meteo API...\n",
            "Total stations: 10\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ New York: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  10%|█         | 1/10 [00:02<00:18,  2.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Los Angeles: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  20%|██        | 2/10 [00:04<00:16,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Chicago: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  30%|███       | 3/10 [00:06<00:14,  2.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Houston: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  40%|████      | 4/10 [00:07<00:11,  1.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Phoenix: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  50%|█████     | 5/10 [00:09<00:09,  1.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Philadelphia: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  60%|██████    | 6/10 [00:14<00:10,  2.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ San Antonio: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  70%|███████   | 7/10 [00:16<00:07,  2.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ San Diego: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  80%|████████  | 8/10 [00:18<00:04,  2.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dallas: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations:  90%|█████████ | 9/10 [00:20<00:02,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ San Jose: 17544 records\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stations: 100%|██████████| 10/10 [00:22<00:00,  2.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Successfully downloaded: 10/10 stations\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data for all stations\n",
        "all_data = []\n",
        "failed_stations = []\n",
        "\n",
        "print(\"Downloading data from Open-Meteo API...\")\n",
        "print(f\"Total stations: {len(STATIONS)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for station_name, station_info in tqdm(STATIONS.items(), desc=\"Stations\"):\n",
        "    lat = station_info['lat']\n",
        "    lon = station_info['lon']\n",
        "    \n",
        "    df = download_openmeteo_data(\n",
        "        latitude=lat,\n",
        "        longitude=lon,\n",
        "        start_date=START_DATE,\n",
        "        end_date=END_DATE,\n",
        "        station_name=station_name\n",
        "    )\n",
        "    \n",
        "    if df is not None and len(df) > 0:\n",
        "        all_data.append(df)\n",
        "        print(f\"✓ {station_name}: {len(df)} records\")\n",
        "    else:\n",
        "        failed_stations.append(station_name)\n",
        "        print(f\"✗ {station_name}: Failed to download\")\n",
        "    \n",
        "    # Rate limiting: be respectful to the API\n",
        "    time.sleep(0.5)  # 0.5 second delay between requests\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Successfully downloaded: {len(all_data)}/{len(STATIONS)} stations\")\n",
        "if failed_stations:\n",
        "    print(f\"Failed stations: {failed_stations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Combine and Validate Data\n",
        "\n",
        "Combine all station data and perform basic validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 175,440\n",
            "Date range: 2024-02-05 00:00:00 to 2026-02-04 23:00:00\n",
            "Number of stations: 10\n",
            "\n",
            "Stations: New York, Los Angeles, Chicago, Houston, Phoenix, Philadelphia, San Antonio, San Diego, Dallas, San Jose\n",
            "\n",
            "==================================================\n",
            "Data Summary\n",
            "==================================================\n",
            "<class 'pandas.DataFrame'>\n",
            "RangeIndex: 175440 entries, 0 to 175439\n",
            "Data columns (total 10 columns):\n",
            " #   Column                Non-Null Count   Dtype         \n",
            "---  ------                --------------   -----         \n",
            " 0   timestamp             175440 non-null  datetime64[us]\n",
            " 1   temperature_2m        175440 non-null  float64       \n",
            " 2   relative_humidity_2m  175440 non-null  int64         \n",
            " 3   dewpoint_2m           175440 non-null  float64       \n",
            " 4   wind_speed_10m        175440 non-null  float64       \n",
            " 5   wind_direction_10m    175440 non-null  int64         \n",
            " 6   surface_pressure      175440 non-null  float64       \n",
            " 7   latitude              175440 non-null  float64       \n",
            " 8   longitude             175440 non-null  float64       \n",
            " 9   station_name          175440 non-null  str           \n",
            "dtypes: datetime64[us](1), float64(6), int64(2), str(1)\n",
            "memory usage: 14.8 MB\n",
            "None\n",
            "\n",
            "==================================================\n",
            "Missing Values\n",
            "==================================================\n",
            "Empty DataFrame\n",
            "Columns: [Missing Count, Missing Percentage]\n",
            "Index: []\n",
            "\n",
            "==================================================\n",
            "Statistical Summary\n",
            "==================================================\n",
            "                        timestamp  temperature_2m  relative_humidity_2m  \\\n",
            "count                      175440   175440.000000         175440.000000   \n",
            "mean   2025-02-04 11:29:59.999999       17.679075             64.707672   \n",
            "min           2024-02-05 00:00:00      -23.000000              2.000000   \n",
            "25%           2024-08-05 17:45:00       11.900000             49.000000   \n",
            "50%           2025-02-04 11:30:00       18.000000             68.000000   \n",
            "75%           2025-08-06 05:15:00       24.400000             84.000000   \n",
            "max           2026-02-04 23:00:00       46.900000            100.000000   \n",
            "std                           NaN        9.685763             22.805182   \n",
            "\n",
            "         dewpoint_2m  wind_speed_10m  wind_direction_10m  surface_pressure  \\\n",
            "count  175440.000000   175440.000000       175440.000000     175440.000000   \n",
            "mean        9.565714        9.759290          187.356019       1002.529165   \n",
            "min       -33.800000        0.000000            0.000000        953.700000   \n",
            "25%         3.900000        5.000000          114.000000        993.000000   \n",
            "50%        10.900000        8.700000          185.000000       1006.200000   \n",
            "75%        16.400000       13.300000          270.000000       1013.100000   \n",
            "max        27.000000       75.700000          360.000000       1045.000000   \n",
            "std         9.282420        6.225233           97.775152         14.209110   \n",
            "\n",
            "            latitude      longitude  \n",
            "count  175440.000000  175440.000000  \n",
            "mean       35.205920     -99.682650  \n",
            "min        29.424100    -121.886300  \n",
            "25%        32.715700    -117.161100  \n",
            "50%        33.750300     -97.645300  \n",
            "75%        39.952600     -87.629800  \n",
            "max        41.878100     -74.006000  \n",
            "std         4.260607      16.524468  \n"
          ]
        }
      ],
      "source": [
        "# Combine all data\n",
        "if all_data:\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    print(f\"Total records: {len(combined_df):,}\")\n",
        "    print(f\"Date range: {combined_df['timestamp'].min()} to {combined_df['timestamp'].max()}\")\n",
        "    print(f\"Number of stations: {combined_df['station_name'].nunique()}\")\n",
        "    print(f\"\\nStations: {', '.join(combined_df['station_name'].unique())}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Data Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(combined_df.info())\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Missing Values\")\n",
        "    print(\"=\" * 50)\n",
        "    missing = combined_df.isnull().sum()\n",
        "    missing_pct = (missing / len(combined_df)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing,\n",
        "        'Missing Percentage': missing_pct\n",
        "    })\n",
        "    print(missing_df[missing_df['Missing Count'] > 0])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Statistical Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(combined_df.describe())\n",
        "else:\n",
        "    print(\"No data downloaded. Please check the API connection and station locations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Data\n",
        "\n",
        "Save the downloaded data for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\openmeteo_raw_data.parquet\n",
            "CSV backup saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\openmeteo_raw_data.csv\n",
            "Station metadata saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\openmeteo_station_metadata.csv\n",
            "\n",
            "✓ Data download complete!\n"
          ]
        }
      ],
      "source": [
        "if all_data:\n",
        "    # Save raw data\n",
        "    raw_data_file = RAW_DATA_DIR / \"openmeteo_raw_data.parquet\"\n",
        "    combined_df.to_parquet(raw_data_file, index=False)\n",
        "    print(f\"Raw data saved to: {raw_data_file}\")\n",
        "    \n",
        "    # Also save as CSV for easy inspection\n",
        "    csv_file = RAW_DATA_DIR / \"openmeteo_raw_data.csv\"\n",
        "    combined_df.to_csv(csv_file, index=False)\n",
        "    print(f\"CSV backup saved to: {csv_file}\")\n",
        "    \n",
        "    # Save station metadata\n",
        "    station_metadata = []\n",
        "    for name, info in STATIONS.items():\n",
        "        if name in combined_df['station_name'].values:\n",
        "            station_metadata.append({\n",
        "                'station_name': name,\n",
        "                'latitude': info['lat'],\n",
        "                'longitude': info['lon'],\n",
        "                'elevation': info['elevation']\n",
        "            })\n",
        "    \n",
        "    metadata_df = pd.DataFrame(station_metadata)\n",
        "    metadata_file = RAW_DATA_DIR / \"openmeteo_station_metadata.csv\"\n",
        "    metadata_df.to_csv(metadata_file, index=False)\n",
        "    print(f\"Station metadata saved to: {metadata_file}\")\n",
        "    \n",
        "    print(\"\\n✓ Data download complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-earth-sgnn (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
