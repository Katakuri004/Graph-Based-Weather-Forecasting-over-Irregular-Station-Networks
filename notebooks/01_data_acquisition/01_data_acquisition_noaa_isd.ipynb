{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Acquisition: NOAA Integrated Surface Database (ISD)\n",
        "\n",
        "This notebook loads operational-quality hourly surface observations from NOAA ISD for the primary production dataset.\n",
        "\n",
        "## Objectives\n",
        "1. Load station inventory and metadata\n",
        "2. Load hourly observations for selected stations\n",
        "3. Parse NOAA Global Hourly CSV format\n",
        "4. Extract key meteorological variables (temperature, humidity, wind, pressure)\n",
        "5. Handle missing data and station temporal coverage\n",
        "6. Create cleaned dataset for model training\n",
        "\n",
        "## Data Source\n",
        "- **NOAA ISD**: Integrated Surface Database (Global Hourly)\n",
        "- **Access**: Direct download from https://www.ncei.noaa.gov/data/global-hourly/access/\n",
        "- **Format**: CSV files (one per station per year)\n",
        "- **Coverage**: Global land stations, hourly observations\n",
        "- **Variables**: 2m air temperature, relative humidity, dew point, wind speed/direction, surface pressure\n",
        "\n",
        "## Two Paths to Load Data\n",
        "\n",
        "### Option A: Load Pre-Downloaded CSV Files (Recommended)\n",
        "If you've already downloaded CSV files from the NOAA website to `data/noaa-data/`:\n",
        "1. Run cells 1-2 (imports and configuration)\n",
        "2. **Skip to Section 3** and run cells 6-9 to load your local CSV files\n",
        "3. Continue with Section 5 (Data Validation)\n",
        "\n",
        "### Option B: Download via FTP (if network allows)\n",
        "If you want to download data programmatically:\n",
        "1. Run cells 1-5 for FTP-based download\n",
        "2. Note: This may fail due to network restrictions\n",
        "\n",
        "## Alternative: Standalone Download Script\n",
        "\n",
        "If you encounter persistent network issues, use the standalone script:\n",
        "\n",
        "```bash\n",
        "python scripts/download_noaa_isd_data.py \\\n",
        "    --stations 725030-14732,722950-23174 \\\n",
        "    --years 2022,2023 \\\n",
        "    --output-dir data/raw/noaa_isd\n",
        "```\n",
        "\n",
        "See `scripts/README.md` for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: C:\\Users\\Kata\\Desktop\\earth-sgnn\n",
            "Raw data directory: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\n",
            "Processed data directory: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import ftplib\n",
        "import gzip\n",
        "import io\n",
        "import re\n",
        "import requests\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "\n",
        "# Setup project path\n",
        "current_dir = Path(os.getcwd()).resolve()\n",
        "\n",
        "# Find project root by looking for src/ and notebooks/ directories\n",
        "if (current_dir / 'src').exists() and (current_dir / 'notebooks').exists():\n",
        "    project_root = current_dir\n",
        "elif current_dir.name in ['01_data_acquisition', '02_data_preprocessing', '03_baselines', \n",
        "                           '04_gnn_models', '05_training', '06_analysis', '07_evaluation', '08_documentation']:\n",
        "    project_root = current_dir.parent.parent\n",
        "elif current_dir.name == 'notebooks':\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    # Walk up to find project root\n",
        "    for parent in current_dir.parents:\n",
        "        if (parent / 'src').exists() and (parent / 'notebooks').exists():\n",
        "            project_root = parent\n",
        "            break\n",
        "    else:\n",
        "        project_root = current_dir\n",
        "\n",
        "# Add project root to Python path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Now we can import from src\n",
        "from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
        "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. NOAA ISD Configuration\n",
        "\n",
        "NOAA ISD data is accessed via FTP. We'll download:\n",
        "- Station inventory (station metadata)\n",
        "- Hourly observation files (fixed-width format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start date: 2022-01-01\n",
            "End date: 2023-12-31\n",
            "Years to download: 2022 to 2023\n",
            "Total days: 729\n",
            "Target region: US\n"
          ]
        }
      ],
      "source": [
        "# NOAA ISD FTP Configuration\n",
        "NOAA_ISD_FTP_HOST = \"ftp.ncei.noaa.gov\"\n",
        "NOAA_ISD_FTP_PATH = \"/pub/data/noaa/\"\n",
        "\n",
        "# Date range for data download\n",
        "# Note: ISD data is typically available with a few days delay\n",
        "# Only download data for years that exist (current year and past years)\n",
        "CURRENT_YEAR = datetime.now().year\n",
        "# ISD data is typically available up to a few days ago, so use current year - 1 as max\n",
        "# For prototyping, let's use 2022-2023 (recent years with complete data)\n",
        "END_YEAR = min(CURRENT_YEAR - 1, 2023)  # Don't go beyond available data\n",
        "START_YEAR = END_YEAR - 1  # 2 years of data\n",
        "\n",
        "END_DATE = datetime(END_YEAR, 12, 31)  # End of the last complete year\n",
        "START_DATE = datetime(START_YEAR, 1, 1)  # Start of first year\n",
        "\n",
        "# Target region (can be expanded)\n",
        "# For initial download, we'll focus on US stations\n",
        "TARGET_REGION = \"US\"  # Options: \"US\", \"global\"\n",
        "\n",
        "print(f\"Start date: {START_DATE.strftime('%Y-%m-%d')}\")\n",
        "print(f\"End date: {END_DATE.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Years to download: {START_YEAR} to {END_YEAR}\")\n",
        "print(f\"Total days: {(END_DATE - START_DATE).days}\")\n",
        "print(f\"Target region: {TARGET_REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Station Inventory\n",
        "\n",
        "The station inventory contains metadata for all ISD stations including:\n",
        "- Station ID (USAF-WBAN)\n",
        "- Station name\n",
        "- Latitude, longitude, elevation\n",
        "- Country code\n",
        "- Temporal coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Downloading NOAA ISD Station Inventory\n",
            "============================================================\n",
            "Connecting to NOAA ISD FTP server...\n",
            "Downloading station inventory: isd-inventory.txt.z\n"
          ]
        }
      ],
      "source": [
        "def download_station_inventory(output_file: Optional[Path] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download NOAA ISD station inventory from FTP.\n",
        "    \n",
        "    The inventory file is a fixed-width format file containing station metadata.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_file : Path, optional\n",
        "        Path to save the inventory file. If None, saves to RAW_DATA_DIR.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with station metadata\n",
        "    \"\"\"\n",
        "    if output_file is None:\n",
        "        output_file = RAW_DATA_DIR / \"noaa_isd_station_inventory.csv\"\n",
        "    \n",
        "    print(\"Connecting to NOAA ISD FTP server...\")\n",
        "    \n",
        "    try:\n",
        "        ftp = ftplib.FTP(NOAA_ISD_FTP_HOST)\n",
        "        ftp.login()  # Anonymous login\n",
        "        ftp.cwd(NOAA_ISD_FTP_PATH)\n",
        "        \n",
        "        # List files to find inventory\n",
        "        files = ftp.nlst()\n",
        "        inventory_file = None\n",
        "        \n",
        "        # Look for inventory file (typically named isd-history.csv or similar)\n",
        "        for f in files:\n",
        "            if 'history' in f.lower() or 'inventory' in f.lower() or 'station' in f.lower():\n",
        "                inventory_file = f\n",
        "                break\n",
        "        \n",
        "        if inventory_file is None:\n",
        "            # Try common names\n",
        "            for name in ['isd-history.csv', 'isd-history.txt', 'isd-stations.txt']:\n",
        "                try:\n",
        "                    ftp.size(name)\n",
        "                    inventory_file = name\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        if inventory_file is None:\n",
        "            print(\"Warning: Could not find station inventory file automatically.\")\n",
        "            print(\"Available files:\", files[:10])  # Show first 10\n",
        "            return None\n",
        "        \n",
        "        print(f\"Downloading station inventory: {inventory_file}\")\n",
        "        \n",
        "        # Download file\n",
        "        buffer = io.BytesIO()\n",
        "        ftp.retrbinary(f'RETR {inventory_file}', buffer.write)\n",
        "        buffer.seek(0)\n",
        "        \n",
        "        ftp.quit()\n",
        "        \n",
        "        # Handle .z compressed files (Unix compress format)\n",
        "        if inventory_file.endswith('.z'):\n",
        "            try:\n",
        "                import zlib\n",
        "                # Read compressed data\n",
        "                compressed_data = buffer.read()\n",
        "                # Try to decompress (zlib can handle some .z files, but not all)\n",
        "                # For .z files, we might need a different approach\n",
        "                # Let's try using gzip first (some .z files are actually gzip)\n",
        "                buffer.seek(0)\n",
        "                try:\n",
        "                    import gzip\n",
        "                    decompressed = gzip.decompress(compressed_data)\n",
        "                    buffer = io.BytesIO(decompressed)\n",
        "                except:\n",
        "                    # If gzip fails, try zlib\n",
        "                    try:\n",
        "                        # Skip header if present and try decompression\n",
        "                        decompressed = zlib.decompress(compressed_data)\n",
        "                        buffer = io.BytesIO(decompressed)\n",
        "                    except:\n",
        "                        print(\"Warning: Could not decompress .z file. Trying alternative method...\")\n",
        "                        # Save to temp file and use system decompress if available\n",
        "                        temp_file = RAW_DATA_DIR / \"temp_inventory.z\"\n",
        "                        with open(temp_file, 'wb') as f:\n",
        "                            f.write(compressed_data)\n",
        "                        print(f\"Saved compressed file to {temp_file}\")\n",
        "                        print(\"Note: You may need to decompress manually or install a .z decompression tool\")\n",
        "                        return None\n",
        "            except ImportError:\n",
        "                print(\"Warning: zlib not available. Cannot decompress .z file.\")\n",
        "                return None\n",
        "        \n",
        "        # Parse fixed-width format\n",
        "        # ISD inventory format: USAF,WBAN,STATION NAME,CTRY,STATE,ICAO,LAT,LON,ELEV(M),BEGIN,END\n",
        "        # Note: Format may vary, we'll try to parse it flexibly\n",
        "        try:\n",
        "            # Try reading as CSV first\n",
        "            df = pd.read_csv(buffer, encoding='utf-8', low_memory=False)\n",
        "        except:\n",
        "            buffer.seek(0)\n",
        "            # Try with different encoding\n",
        "            try:\n",
        "                df = pd.read_csv(buffer, encoding='latin-1', low_memory=False)\n",
        "            except:\n",
        "                buffer.seek(0)\n",
        "                # Try reading as fixed-width text\n",
        "                try:\n",
        "                    content = buffer.read().decode('latin-1', errors='ignore')\n",
        "                    lines = content.split('\\n')\n",
        "                    # ISD inventory is typically fixed-width or CSV\n",
        "                    # Try parsing as CSV with different separators\n",
        "                    if len(lines) > 0:\n",
        "                        # Check if it's CSV-like\n",
        "                        if ',' in lines[0] or '\\t' in lines[0]:\n",
        "                            sep = ',' if ',' in lines[0] else '\\t'\n",
        "                            df = pd.read_csv(io.StringIO(content), sep=sep, encoding='latin-1', \n",
        "                                            low_memory=False, on_bad_lines='skip')\n",
        "                        else:\n",
        "                            # Fixed-width format - parse manually\n",
        "                            print(\"Attempting fixed-width parsing...\")\n",
        "                            # ISD inventory format (based on NOAA documentation):\n",
        "                            # USAF: 0-5 (6 chars)\n",
        "                            # WBAN: 7-11 (5 chars)\n",
        "                            # STATION NAME: 13-42 (30 chars)\n",
        "                            # CTRY: 43-44 (2 chars)\n",
        "                            # STATE: 46-47 (2 chars)\n",
        "                            # ICAO: 49-52 (4 chars)\n",
        "                            # LAT: 57-63 (7 chars, +N/-S, decimal degrees)\n",
        "                            # LON: 65-72 (8 chars, +E/-W, decimal degrees)\n",
        "                            # ELEV(M): 74-80 (7 chars)\n",
        "                            # BEGIN: 82-89 (8 chars, YYYYMMDD)\n",
        "                            # END: 91-98 (8 chars, YYYYMMDD)\n",
        "                            data = []\n",
        "                            header_skipped = False\n",
        "                            for line in lines:\n",
        "                                if len(line.strip()) == 0:\n",
        "                                    continue\n",
        "                                \n",
        "                                # Skip header rows (lines that don't start with digits or have \"USAF\" in them)\n",
        "                                if not header_skipped:\n",
        "                                    if 'USAF' in line.upper() or 'FEDERAL' in line.upper() or 'INVENTORY' in line.upper():\n",
        "                                        continue\n",
        "                                    if not (line[0:6].strip().isdigit() or len(line[0:6].strip()) == 0):\n",
        "                                        continue\n",
        "                                    header_skipped = True\n",
        "                                \n",
        "                                if len(line) < 80:  # Need at least 80 chars for full record\n",
        "                                    continue\n",
        "                                \n",
        "                                try:\n",
        "                                    usaf = line[0:6].strip()\n",
        "                                    wban = line[7:12].strip() if len(line) > 12 else ''\n",
        "                                    \n",
        "                                    # Skip if USAF is not a valid station ID\n",
        "                                    if not usaf.isdigit() or len(usaf) == 0:\n",
        "                                        continue\n",
        "                                    \n",
        "                                    name = line[13:43].strip() if len(line) > 43 else ''\n",
        "                                    ctry = line[43:45].strip() if len(line) > 45 else ''\n",
        "                                    state = line[46:48].strip() if len(line) > 48 else ''\n",
        "                                    icao = line[49:53].strip() if len(line) > 53 else ''\n",
        "                                    \n",
        "                                    # Parse latitude (position 57-63, format: +NN.NNN or -NN.NNN)\n",
        "                                    lat_str = line[57:64].strip() if len(line) > 64 else ''\n",
        "                                    lat = np.nan\n",
        "                                    if lat_str:\n",
        "                                        try:\n",
        "                                            lat_val = float(lat_str)\n",
        "                                            lat = lat_val\n",
        "                                        except:\n",
        "                                            pass\n",
        "                                    \n",
        "                                    # Parse longitude (position 65-72, format: +NNN.NNN or -NNN.NNN)\n",
        "                                    lon_str = line[65:73].strip() if len(line) > 73 else ''\n",
        "                                    lon = np.nan\n",
        "                                    if lon_str:\n",
        "                                        try:\n",
        "                                            lon_val = float(lon_str)\n",
        "                                            lon = lon_val\n",
        "                                        except:\n",
        "                                            pass\n",
        "                                    \n",
        "                                    # Parse elevation (position 74-80)\n",
        "                                    elev_str = line[74:81].strip() if len(line) > 81 else ''\n",
        "                                    elev = np.nan\n",
        "                                    if elev_str:\n",
        "                                        try:\n",
        "                                            elev = float(elev_str)\n",
        "                                        except:\n",
        "                                            pass\n",
        "                                    \n",
        "                                    # Parse BEGIN date (position 82-89, YYYYMMDD)\n",
        "                                    begin_str = line[82:90].strip() if len(line) > 90 else ''\n",
        "                                    begin = None\n",
        "                                    if begin_str and len(begin_str) == 8 and begin_str.isdigit():\n",
        "                                        begin = begin_str\n",
        "                                    \n",
        "                                    # Parse END date (position 91-98, YYYYMMDD)\n",
        "                                    end_str = line[91:99].strip() if len(line) > 99 else ''\n",
        "                                    end = None\n",
        "                                    if end_str and len(end_str) == 8 and end_str.isdigit():\n",
        "                                        end = end_str\n",
        "                                    \n",
        "                                    data.append({\n",
        "                                        'USAF': usaf,\n",
        "                                        'WBAN': wban,\n",
        "                                        'STATION NAME': name,\n",
        "                                        'CTRY': ctry,\n",
        "                                        'STATE': state,\n",
        "                                        'ICAO': icao,\n",
        "                                        'LAT': lat,\n",
        "                                        'LON': lon,\n",
        "                                        'ELEV(M)': elev,\n",
        "                                        'BEGIN': begin,\n",
        "                                        'END': end\n",
        "                                    })\n",
        "                                except Exception as e:\n",
        "                                    continue\n",
        "                            \n",
        "                            if len(data) > 0:\n",
        "                                df = pd.DataFrame(data)\n",
        "                                # Filter out rows with invalid coordinates\n",
        "                                df = df[df['LAT'].notna() & df['LON'].notna()]\n",
        "                            else:\n",
        "                                raise ValueError(\"Could not parse fixed-width format\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not automatically parse inventory file: {e}\")\n",
        "                    print(\"Please download manually from: https://www.ncei.noaa.gov/data/global-hourly/access/\")\n",
        "                    return None\n",
        "        \n",
        "        # Clean column names\n",
        "        df.columns = df.columns.str.strip()\n",
        "        \n",
        "        # Save to CSV\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"Station inventory saved to: {output_file}\")\n",
        "        print(f\"Total stations: {len(df)}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading station inventory: {e}\")\n",
        "        print(\"\\nAlternative: Download manually from:\")\n",
        "        print(\"https://www.ncei.noaa.gov/data/global-hourly/access/\")\n",
        "        print(\"or\")\n",
        "        print(\"https://www.ncei.noaa.gov/pub/data/noaa/\")\n",
        "        return None\n",
        "\n",
        "# Download station inventory\n",
        "print(\"=\" * 60)\n",
        "print(\"Downloading NOAA ISD Station Inventory\")\n",
        "print(\"=\" * 60)\n",
        "station_inventory = download_station_inventory()\n",
        "\n",
        "if station_inventory is not None:\n",
        "    print(\"\\nFirst few stations:\")\n",
        "    print(station_inventory.head())\n",
        "    print(f\"\\nColumns: {list(station_inventory.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Pre-Downloaded NOAA Global Hourly Data\n",
        "\n",
        "If you've already downloaded data from https://www.ncei.noaa.gov/data/global-hourly/access/2022/, \n",
        "use this section to load the CSV files directly instead of downloading via FTP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1090 CSV files in C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\noaa-data\n",
            "\n",
            "Sample filenames: ['01001099999.csv', '01001499999.csv', '01002099999.csv', '01003099999.csv', '01006099999.csv']\n"
          ]
        }
      ],
      "source": [
        "# Path to pre-downloaded NOAA Global Hourly CSV files\n",
        "NOAA_LOCAL_DATA_DIR = project_root / \"data\" / \"noaa-data\"\n",
        "\n",
        "def parse_noaa_global_hourly_csv(file_path: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Parse a NOAA Global Hourly CSV file.\n",
        "    \n",
        "    The CSV format has columns like:\n",
        "    - STATION: Combined USAF+WBAN (e.g., \"06279099999\")\n",
        "    - DATE: ISO timestamp (e.g., \"2022-01-01T00:00:00\")\n",
        "    - LATITUDE, LONGITUDE, ELEVATION\n",
        "    - NAME: Station name\n",
        "    - TMP: Temperature \"+NNNN,Q\" (scaled by 10, Celsius)\n",
        "    - DEW: Dew point \"+NNNN,Q\"\n",
        "    - WND: \"DIR,Q,TYPE,SPD,Q\" (direction in degrees, speed in m/s * 10)\n",
        "    - SLP: Sea level pressure \"NNNNN,Q\" (scaled by 10, hPa)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : Path\n",
        "        Path to the CSV file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame or None\n",
        "        Parsed observations with standardized columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, low_memory=False)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error reading {file_path.name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    if len(df) == 0:\n",
        "        return None\n",
        "    \n",
        "    # Parse station info\n",
        "    station_id = df['STATION'].iloc[0] if 'STATION' in df.columns else file_path.stem\n",
        "    \n",
        "    # Parse timestamp\n",
        "    df['timestamp'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    \n",
        "    if len(df) == 0:\n",
        "        return None\n",
        "    \n",
        "    # Parse temperature (TMP format: \"+NNNN,Q\" where NNNN is temp * 10 in Celsius)\n",
        "    def parse_tmp(val):\n",
        "        if pd.isna(val) or val == '' or '+9999' in str(val):\n",
        "            return np.nan\n",
        "        try:\n",
        "            parts = str(val).split(',')\n",
        "            temp_str = parts[0]\n",
        "            temp_val = float(temp_str) / 10.0\n",
        "            if -100 <= temp_val <= 100:\n",
        "                return temp_val\n",
        "        except:\n",
        "            pass\n",
        "        return np.nan\n",
        "    \n",
        "    # Parse dew point (same format as TMP)\n",
        "    def parse_dew(val):\n",
        "        return parse_tmp(val)  # Same parsing logic\n",
        "    \n",
        "    # Parse wind (WND format: \"DIR,Q,TYPE,SPD,Q\")\n",
        "    def parse_wind_direction(val):\n",
        "        if pd.isna(val) or val == '':\n",
        "            return np.nan\n",
        "        try:\n",
        "            parts = str(val).split(',')\n",
        "            if len(parts) >= 1:\n",
        "                dir_val = int(parts[0])\n",
        "                if 0 <= dir_val <= 360 and dir_val != 999:\n",
        "                    return float(dir_val)\n",
        "        except:\n",
        "            pass\n",
        "        return np.nan\n",
        "    \n",
        "    def parse_wind_speed(val):\n",
        "        if pd.isna(val) or val == '':\n",
        "            return np.nan\n",
        "        try:\n",
        "            parts = str(val).split(',')\n",
        "            if len(parts) >= 4:\n",
        "                speed_str = parts[3]\n",
        "                speed_val = float(speed_str) / 10.0  # Convert from m/s * 10\n",
        "                if 0 <= speed_val <= 200 and speed_str != '9999':\n",
        "                    return speed_val\n",
        "        except:\n",
        "            pass\n",
        "        return np.nan\n",
        "    \n",
        "    # Parse sea level pressure (SLP format: \"NNNNN,Q\" where NNNNN is pressure * 10 in hPa)\n",
        "    def parse_slp(val):\n",
        "        if pd.isna(val) or val == '' or '99999' in str(val):\n",
        "            return np.nan\n",
        "        try:\n",
        "            parts = str(val).split(',')\n",
        "            press_str = parts[0]\n",
        "            press_val = float(press_str) / 10.0\n",
        "            if 800 <= press_val <= 1100:\n",
        "                return press_val\n",
        "        except:\n",
        "            pass\n",
        "        return np.nan\n",
        "    \n",
        "    # Apply parsing\n",
        "    result = pd.DataFrame({\n",
        "        'station_id': station_id,\n",
        "        'timestamp': df['timestamp'],\n",
        "        'latitude': df['LATITUDE'].iloc[0] if 'LATITUDE' in df.columns else np.nan,\n",
        "        'longitude': df['LONGITUDE'].iloc[0] if 'LONGITUDE' in df.columns else np.nan,\n",
        "        'elevation': df['ELEVATION'].iloc[0] if 'ELEVATION' in df.columns else np.nan,\n",
        "        'name': df['NAME'].iloc[0] if 'NAME' in df.columns else '',\n",
        "        'temperature_2m': df['TMP'].apply(parse_tmp) if 'TMP' in df.columns else np.nan,\n",
        "        'dewpoint_2m': df['DEW'].apply(parse_dew) if 'DEW' in df.columns else np.nan,\n",
        "        'wind_direction_10m': df['WND'].apply(parse_wind_direction) if 'WND' in df.columns else np.nan,\n",
        "        'wind_speed_10m': df['WND'].apply(parse_wind_speed) if 'WND' in df.columns else np.nan,\n",
        "        'surface_pressure': df['SLP'].apply(parse_slp) if 'SLP' in df.columns else np.nan\n",
        "    })\n",
        "    \n",
        "    # Calculate relative humidity from temperature and dew point\n",
        "    def calc_rh(row):\n",
        "        T = row['temperature_2m']\n",
        "        Td = row['dewpoint_2m']\n",
        "        if pd.isna(T) or pd.isna(Td):\n",
        "            return np.nan\n",
        "        try:\n",
        "            e_sat = 6.112 * np.exp(17.67 * T / (T + 243.5))\n",
        "            e_act = 6.112 * np.exp(17.67 * Td / (Td + 243.5))\n",
        "            rh = 100.0 * (e_act / e_sat)\n",
        "            if 0 <= rh <= 100:\n",
        "                return rh\n",
        "        except:\n",
        "            pass\n",
        "        return np.nan\n",
        "    \n",
        "    result['relative_humidity_2m'] = result.apply(calc_rh, axis=1)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Check if local data exists\n",
        "if NOAA_LOCAL_DATA_DIR.exists():\n",
        "    csv_files = list(NOAA_LOCAL_DATA_DIR.glob(\"*.csv\"))\n",
        "    print(f\"Found {len(csv_files)} CSV files in {NOAA_LOCAL_DATA_DIR}\")\n",
        "    \n",
        "    if len(csv_files) > 0:\n",
        "        print(f\"\\nSample filenames: {[f.name for f in csv_files[:5]]}\")\n",
        "else:\n",
        "    csv_files = []\n",
        "    print(f\"Local data directory not found: {NOAA_LOCAL_DATA_DIR}\")\n",
        "    print(\"Please download data from: https://www.ncei.noaa.gov/data/global-hourly/access/2022/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Loading 1090 NOAA Global Hourly CSV files...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading CSV files: 100%|██████████| 1090/1090 [03:14<00:00,  5.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Successfully loaded data from 1090 stations\n",
            "✓ Total observations: 9,612,898\n",
            "✓ Date range: 2022-01-01 00:00:00 to 2022-12-31 23:59:00\n"
          ]
        }
      ],
      "source": [
        "# Load all local CSV files\n",
        "if csv_files and len(csv_files) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Loading {len(csv_files)} NOAA Global Hourly CSV files...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_data = []\n",
        "    station_metadata = []\n",
        "    failed_files = []\n",
        "    \n",
        "    # Process files with progress bar\n",
        "    for file_path in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
        "        try:\n",
        "            df = parse_noaa_global_hourly_csv(file_path)\n",
        "            if df is not None and len(df) > 0:\n",
        "                all_data.append(df)\n",
        "                # Extract metadata for first observation\n",
        "                station_metadata.append({\n",
        "                    'station_id': df['station_id'].iloc[0],\n",
        "                    'name': df['name'].iloc[0],\n",
        "                    'latitude': df['latitude'].iloc[0],\n",
        "                    'longitude': df['longitude'].iloc[0],\n",
        "                    'elevation': df['elevation'].iloc[0],\n",
        "                    'n_observations': len(df),\n",
        "                    'start_date': df['timestamp'].min(),\n",
        "                    'end_date': df['timestamp'].max()\n",
        "                })\n",
        "        except Exception as e:\n",
        "            failed_files.append((file_path.name, str(e)))\n",
        "    \n",
        "    if all_data:\n",
        "        # Combine all data\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        selected_stations = pd.DataFrame(station_metadata)\n",
        "        \n",
        "        print(f\"\\n✓ Successfully loaded data from {len(all_data)} stations\")\n",
        "        print(f\"✓ Total observations: {len(combined_df):,}\")\n",
        "        print(f\"✓ Date range: {combined_df['timestamp'].min()} to {combined_df['timestamp'].max()}\")\n",
        "        \n",
        "        if failed_files:\n",
        "            print(f\"\\n⚠ Failed to parse {len(failed_files)} files\")\n",
        "            if len(failed_files) <= 5:\n",
        "                for fname, err in failed_files:\n",
        "                    print(f\"  - {fname}: {err}\")\n",
        "    else:\n",
        "        print(\"\\nNo data could be loaded from CSV files.\")\n",
        "        combined_df = None\n",
        "        selected_stations = None\n",
        "else:\n",
        "    print(\"\\nNo CSV files found. Please download data first.\")\n",
        "    combined_df = None\n",
        "    selected_stations = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Data Preview\n",
            "============================================================\n",
            "\n",
            "Sample observations:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>station_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>elevation</th>\n",
              "      <th>name</th>\n",
              "      <th>temperature_2m</th>\n",
              "      <th>dewpoint_2m</th>\n",
              "      <th>wind_direction_10m</th>\n",
              "      <th>wind_speed_10m</th>\n",
              "      <th>surface_pressure</th>\n",
              "      <th>relative_humidity_2m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 01:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-8.7</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>358.0</td>\n",
              "      <td>13.9</td>\n",
              "      <td>1014.0</td>\n",
              "      <td>82.758801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 02:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>-11.4</td>\n",
              "      <td>353.0</td>\n",
              "      <td>12.7</td>\n",
              "      <td>1014.1</td>\n",
              "      <td>82.718506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 05:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-9.9</td>\n",
              "      <td>-12.7</td>\n",
              "      <td>350.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>1013.8</td>\n",
              "      <td>79.975301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 07:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-10.5</td>\n",
              "      <td>-14.1</td>\n",
              "      <td>351.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>1013.4</td>\n",
              "      <td>74.841808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 08:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-10.5</td>\n",
              "      <td>-15.8</td>\n",
              "      <td>349.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>1013.2</td>\n",
              "      <td>65.062245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 09:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-10.7</td>\n",
              "      <td>-17.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.8</td>\n",
              "      <td>1013.2</td>\n",
              "      <td>58.317554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 10:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-11.7</td>\n",
              "      <td>-15.8</td>\n",
              "      <td>343.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>1013.4</td>\n",
              "      <td>71.589050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 11:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-11.5</td>\n",
              "      <td>-16.2</td>\n",
              "      <td>344.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1013.3</td>\n",
              "      <td>68.148376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 12:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-12.4</td>\n",
              "      <td>-17.6</td>\n",
              "      <td>340.0</td>\n",
              "      <td>8.2</td>\n",
              "      <td>1012.5</td>\n",
              "      <td>65.143940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>2022-01-01 13:00:00</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>-12.5</td>\n",
              "      <td>-19.3</td>\n",
              "      <td>340.0</td>\n",
              "      <td>7.4</td>\n",
              "      <td>1011.9</td>\n",
              "      <td>56.839831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   station_id           timestamp   latitude  longitude  elevation  \\\n",
              "0  1001099999 2022-01-01 01:00:00  70.933333  -8.666667        9.0   \n",
              "1  1001099999 2022-01-01 02:00:00  70.933333  -8.666667        9.0   \n",
              "2  1001099999 2022-01-01 05:00:00  70.933333  -8.666667        9.0   \n",
              "3  1001099999 2022-01-01 07:00:00  70.933333  -8.666667        9.0   \n",
              "4  1001099999 2022-01-01 08:00:00  70.933333  -8.666667        9.0   \n",
              "5  1001099999 2022-01-01 09:00:00  70.933333  -8.666667        9.0   \n",
              "6  1001099999 2022-01-01 10:00:00  70.933333  -8.666667        9.0   \n",
              "7  1001099999 2022-01-01 11:00:00  70.933333  -8.666667        9.0   \n",
              "8  1001099999 2022-01-01 12:00:00  70.933333  -8.666667        9.0   \n",
              "9  1001099999 2022-01-01 13:00:00  70.933333  -8.666667        9.0   \n",
              "\n",
              "                     name  temperature_2m  dewpoint_2m  wind_direction_10m  \\\n",
              "0  JAN MAYEN NOR NAVY, NO            -8.7        -11.1               358.0   \n",
              "1  JAN MAYEN NOR NAVY, NO            -9.0        -11.4               353.0   \n",
              "2  JAN MAYEN NOR NAVY, NO            -9.9        -12.7               350.0   \n",
              "3  JAN MAYEN NOR NAVY, NO           -10.5        -14.1               351.0   \n",
              "4  JAN MAYEN NOR NAVY, NO           -10.5        -15.8               349.0   \n",
              "5  JAN MAYEN NOR NAVY, NO           -10.7        -17.3                 2.0   \n",
              "6  JAN MAYEN NOR NAVY, NO           -11.7        -15.8               343.0   \n",
              "7  JAN MAYEN NOR NAVY, NO           -11.5        -16.2               344.0   \n",
              "8  JAN MAYEN NOR NAVY, NO           -12.4        -17.6               340.0   \n",
              "9  JAN MAYEN NOR NAVY, NO           -12.5        -19.3               340.0   \n",
              "\n",
              "   wind_speed_10m  surface_pressure  relative_humidity_2m  \n",
              "0            13.9            1014.0             82.758801  \n",
              "1            12.7            1014.1             82.718506  \n",
              "2            10.5            1013.8             79.975301  \n",
              "3             8.5            1013.4             74.841808  \n",
              "4             7.9            1013.2             65.062245  \n",
              "5             6.8            1013.2             58.317554  \n",
              "6             8.9            1013.4             71.589050  \n",
              "7             6.0            1013.3             68.148376  \n",
              "8             8.2            1012.5             65.143940  \n",
              "9             7.4            1011.9             56.839831  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Column data types:\n",
            "station_id                       int64\n",
            "timestamp               datetime64[us]\n",
            "latitude                       float64\n",
            "longitude                      float64\n",
            "elevation                      float64\n",
            "name                               str\n",
            "temperature_2m                 float64\n",
            "dewpoint_2m                    float64\n",
            "wind_direction_10m             float64\n",
            "wind_speed_10m                 float64\n",
            "surface_pressure               float64\n",
            "relative_humidity_2m           float64\n",
            "dtype: object\n",
            "\n",
            "Station metadata sample:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>station_id</th>\n",
              "      <th>name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>elevation</th>\n",
              "      <th>n_observations</th>\n",
              "      <th>start_date</th>\n",
              "      <th>end_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001099999</td>\n",
              "      <td>JAN MAYEN NOR NAVY, NO</td>\n",
              "      <td>70.933333</td>\n",
              "      <td>-8.666667</td>\n",
              "      <td>9.00</td>\n",
              "      <td>4022</td>\n",
              "      <td>2022-01-01 01:00:00</td>\n",
              "      <td>2022-12-31 23:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001499999</td>\n",
              "      <td>SORSTOKKEN, NO</td>\n",
              "      <td>59.791925</td>\n",
              "      <td>5.340850</td>\n",
              "      <td>48.76</td>\n",
              "      <td>6216</td>\n",
              "      <td>2022-01-02 11:20:00</td>\n",
              "      <td>2022-12-30 18:20:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1002099999</td>\n",
              "      <td>VERLEGENHUKEN, NO</td>\n",
              "      <td>80.050000</td>\n",
              "      <td>16.250000</td>\n",
              "      <td>8.00</td>\n",
              "      <td>99</td>\n",
              "      <td>2022-01-08 21:00:00</td>\n",
              "      <td>2022-12-31 20:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1003099999</td>\n",
              "      <td>HORNSUND, NO</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>12.00</td>\n",
              "      <td>237</td>\n",
              "      <td>2022-01-02 06:00:00</td>\n",
              "      <td>2022-12-31 06:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1006099999</td>\n",
              "      <td>EDGEOYA, NO</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>22.816667</td>\n",
              "      <td>14.00</td>\n",
              "      <td>123</td>\n",
              "      <td>2022-01-01 17:00:00</td>\n",
              "      <td>2022-12-29 21:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1007099999</td>\n",
              "      <td>NY ALESUND, SV</td>\n",
              "      <td>78.916667</td>\n",
              "      <td>11.933333</td>\n",
              "      <td>7.70</td>\n",
              "      <td>977</td>\n",
              "      <td>2022-01-01 12:00:00</td>\n",
              "      <td>2022-12-31 21:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1008099999</td>\n",
              "      <td>LONGYEAR, SV</td>\n",
              "      <td>78.246111</td>\n",
              "      <td>15.465556</td>\n",
              "      <td>26.82</td>\n",
              "      <td>18125</td>\n",
              "      <td>2022-01-01 00:20:00</td>\n",
              "      <td>2022-12-31 23:50:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1009099999</td>\n",
              "      <td>KARL XII OYA, SV</td>\n",
              "      <td>80.650000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>5.00</td>\n",
              "      <td>133</td>\n",
              "      <td>2022-01-03 21:00:00</td>\n",
              "      <td>2022-12-31 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1010099999</td>\n",
              "      <td>ANDOYA, NO</td>\n",
              "      <td>69.292500</td>\n",
              "      <td>16.144167</td>\n",
              "      <td>13.10</td>\n",
              "      <td>19535</td>\n",
              "      <td>2022-01-01 00:20:00</td>\n",
              "      <td>2022-12-31 23:50:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1011099999</td>\n",
              "      <td>KVITOYA, SV</td>\n",
              "      <td>80.066667</td>\n",
              "      <td>31.500000</td>\n",
              "      <td>10.00</td>\n",
              "      <td>80</td>\n",
              "      <td>2022-01-03 22:00:00</td>\n",
              "      <td>2022-12-31 23:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   station_id                    name   latitude  longitude  elevation  \\\n",
              "0  1001099999  JAN MAYEN NOR NAVY, NO  70.933333  -8.666667       9.00   \n",
              "1  1001499999          SORSTOKKEN, NO  59.791925   5.340850      48.76   \n",
              "2  1002099999       VERLEGENHUKEN, NO  80.050000  16.250000       8.00   \n",
              "3  1003099999            HORNSUND, NO  77.000000  15.500000      12.00   \n",
              "4  1006099999             EDGEOYA, NO  78.250000  22.816667      14.00   \n",
              "5  1007099999          NY ALESUND, SV  78.916667  11.933333       7.70   \n",
              "6  1008099999            LONGYEAR, SV  78.246111  15.465556      26.82   \n",
              "7  1009099999        KARL XII OYA, SV  80.650000  25.000000       5.00   \n",
              "8  1010099999              ANDOYA, NO  69.292500  16.144167      13.10   \n",
              "9  1011099999             KVITOYA, SV  80.066667  31.500000      10.00   \n",
              "\n",
              "   n_observations          start_date            end_date  \n",
              "0            4022 2022-01-01 01:00:00 2022-12-31 23:00:00  \n",
              "1            6216 2022-01-02 11:20:00 2022-12-30 18:20:00  \n",
              "2              99 2022-01-08 21:00:00 2022-12-31 20:00:00  \n",
              "3             237 2022-01-02 06:00:00 2022-12-31 06:00:00  \n",
              "4             123 2022-01-01 17:00:00 2022-12-29 21:00:00  \n",
              "5             977 2022-01-01 12:00:00 2022-12-31 21:00:00  \n",
              "6           18125 2022-01-01 00:20:00 2022-12-31 23:50:00  \n",
              "7             133 2022-01-03 21:00:00 2022-12-31 00:00:00  \n",
              "8           19535 2022-01-01 00:20:00 2022-12-31 23:50:00  \n",
              "9              80 2022-01-03 22:00:00 2022-12-31 23:00:00  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Preview the loaded data\n",
        "if combined_df is not None and len(combined_df) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Data Preview\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\nSample observations:\")\n",
        "    display(combined_df.head(10))\n",
        "    \n",
        "    print(\"\\nColumn data types:\")\n",
        "    print(combined_df.dtypes)\n",
        "    \n",
        "    print(\"\\nStation metadata sample:\")\n",
        "    if selected_stations is not None:\n",
        "        display(selected_stations.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data already loaded: 9,612,898 observations\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RESTORE DATA FROM LOCAL CSV FILES (if FTP download cell set combined_df to None)\n",
        "# ============================================================================\n",
        "# This cell fixes the issue where the FTP download cell overwrites combined_df\n",
        "\n",
        "# Check if we need to reload from the CSV files\n",
        "_need_reload = False\n",
        "try:\n",
        "    if combined_df is None or len(combined_df) == 0:\n",
        "        _need_reload = True\n",
        "except NameError:\n",
        "    _need_reload = True\n",
        "\n",
        "if _need_reload:\n",
        "    print(\"Checking for pre-loaded CSV data...\")\n",
        "    # Try to reload from the noaa-data directory\n",
        "    _noaa_dir = project_root / \"data\" / \"noaa-data\"\n",
        "    if _noaa_dir.exists():\n",
        "        _csv_files = list(_noaa_dir.glob(\"*.csv\"))\n",
        "        if len(_csv_files) > 0:\n",
        "            print(f\"Found {len(_csv_files)} CSV files - reloading...\")\n",
        "            _all_data = []\n",
        "            _station_meta = []\n",
        "            for _fp in tqdm(_csv_files, desc=\"Reloading CSV files\"):\n",
        "                try:\n",
        "                    _df = parse_noaa_global_hourly_csv(_fp)\n",
        "                    if _df is not None and len(_df) > 0:\n",
        "                        _all_data.append(_df)\n",
        "                        _station_meta.append({\n",
        "                            'station_id': _df['station_id'].iloc[0],\n",
        "                            'name': _df['name'].iloc[0],\n",
        "                            'latitude': _df['latitude'].iloc[0],\n",
        "                            'longitude': _df['longitude'].iloc[0],\n",
        "                            'elevation': _df['elevation'].iloc[0],\n",
        "                            'n_observations': len(_df)\n",
        "                        })\n",
        "                except:\n",
        "                    pass\n",
        "            if _all_data:\n",
        "                combined_df = pd.concat(_all_data, ignore_index=True)\n",
        "                selected_stations = pd.DataFrame(_station_meta)\n",
        "                print(f\"\\n✓ Restored {len(combined_df):,} observations from {len(_all_data)} stations\")\n",
        "            else:\n",
        "                print(\"Could not reload data\")\n",
        "        else:\n",
        "            print(\"No CSV files found in data/noaa-data/\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {_noaa_dir}\")\n",
        "else:\n",
        "    print(f\"✓ Data already loaded: {len(combined_df):,} observations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_stations(\n",
        "    inventory: pd.DataFrame,\n",
        "    region: str = \"US\",\n",
        "    min_lat: Optional[float] = None,\n",
        "    max_lat: Optional[float] = None,\n",
        "    min_lon: Optional[float] = None,\n",
        "    max_lon: Optional[float] = None,\n",
        "    min_elevation: Optional[float] = None,\n",
        "    max_elevation: Optional[float] = None,\n",
        "    start_date: Optional[datetime] = None,\n",
        "    end_date: Optional[datetime] = None,\n",
        "    max_stations: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Select stations from inventory based on criteria.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    inventory : pd.DataFrame\n",
        "        Station inventory DataFrame\n",
        "    region : str\n",
        "        Region filter (e.g., \"US\")\n",
        "    min_lat, max_lat, min_lon, max_lon : float, optional\n",
        "        Geographic bounding box\n",
        "    min_elevation, max_elevation : float, optional\n",
        "        Elevation range\n",
        "    start_date, end_date : datetime, optional\n",
        "        Required temporal coverage\n",
        "    max_stations : int, optional\n",
        "        Maximum number of stations to return\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Filtered station inventory\n",
        "    \"\"\"\n",
        "    df = inventory.copy()\n",
        "    \n",
        "    # Filter by region (if country code column exists)\n",
        "    if region is not None and 'CTRY' in df.columns:\n",
        "        if region == \"US\":\n",
        "            df = df[df['CTRY'] == 'US']\n",
        "        # Add other region filters as needed\n",
        "    \n",
        "    # Geographic filters\n",
        "    if 'LAT' in df.columns:\n",
        "        if min_lat is not None:\n",
        "            df = df[df['LAT'] >= min_lat]\n",
        "        if max_lat is not None:\n",
        "            df = df[df['LAT'] <= max_lat]\n",
        "    \n",
        "    if 'LON' in df.columns:\n",
        "        if min_lon is not None:\n",
        "            df = df[df['LON'] >= min_lon]\n",
        "        if max_lon is not None:\n",
        "            df = df[df['LON'] <= max_lon]\n",
        "    \n",
        "    # Elevation filter\n",
        "    elev_col = None\n",
        "    for col in ['ELEV(M)', 'ELEV', 'ELEVATION', 'ELEV_M']:\n",
        "        if col in df.columns:\n",
        "            elev_col = col\n",
        "            break\n",
        "    \n",
        "    if elev_col:\n",
        "        if min_elevation is not None:\n",
        "            df = df[pd.to_numeric(df[elev_col], errors='coerce') >= min_elevation]\n",
        "        if max_elevation is not None:\n",
        "            df = df[pd.to_numeric(df[elev_col], errors='coerce') <= max_elevation]\n",
        "    \n",
        "    # Temporal coverage filter\n",
        "    if start_date and 'BEGIN' in df.columns:\n",
        "        # Parse BEGIN date and filter\n",
        "        try:\n",
        "            df['BEGIN_DATE'] = pd.to_datetime(df['BEGIN'], format='%Y%m%d', errors='coerce')\n",
        "            df = df[df['BEGIN_DATE'] <= start_date]\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if end_date and 'END' in df.columns:\n",
        "        # Parse END date and filter\n",
        "        try:\n",
        "            df['END_DATE'] = pd.to_datetime(df['END'], format='%Y%m%d', errors='coerce')\n",
        "            df = df[(df['END_DATE'] >= end_date) | (df['END_DATE'].isna())]\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Filter out invalid stations\n",
        "    # Remove stations with invalid coordinates (0,0 or NaN)\n",
        "    if 'LAT' in df.columns and 'LON' in df.columns:\n",
        "        df = df[(df['LAT'] != 0) | (df['LON'] != 0)]  # Remove stations at (0,0)\n",
        "        df = df[df['LAT'].notna() & df['LON'].notna()]  # Remove NaN coordinates\n",
        "        # Remove stations with obviously invalid coordinates\n",
        "        df = df[(df['LAT'].abs() <= 90) & (df['LON'].abs() <= 180)]\n",
        "    \n",
        "    # Remove stations with invalid station names (containing years or numbers that look like data)\n",
        "    if 'STATION NAME' in df.columns:\n",
        "        # Filter out rows where station name looks like data (contains \"2020\", \"2021\", etc.)\n",
        "        df = df[~df['STATION NAME'].astype(str).str.contains(r'^\\d{4}', na=False)]\n",
        "        # Filter out rows where station name is mostly numbers/spaces\n",
        "        df = df[df['STATION NAME'].astype(str).str.len() > 3]\n",
        "    \n",
        "    # Remove stations with invalid USAF/WBAN\n",
        "    if 'USAF' in df.columns:\n",
        "        df = df[df['USAF'].astype(str).str.len() >= 5]  # USAF should be at least 5 digits\n",
        "    \n",
        "    # Limit number of stations\n",
        "    if max_stations and len(df) > max_stations:\n",
        "        df = df.head(max_stations)\n",
        "    \n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# Select stations for initial download\n",
        "# For prototyping, we'll select a subset of US stations\n",
        "if station_inventory is not None and len(station_inventory) > 0:\n",
        "    # First, check what country codes are available\n",
        "    if 'CTRY' in station_inventory.columns:\n",
        "        print(f\"\\nAvailable country codes: {station_inventory['CTRY'].value_counts().head(10)}\")\n",
        "    \n",
        "    selected_stations = select_stations(\n",
        "        station_inventory,\n",
        "        region=\"US\",\n",
        "        max_stations=20  # Start with 20 stations for prototyping\n",
        "    )\n",
        "    \n",
        "    # If no stations selected, try without region filter\n",
        "    if len(selected_stations) == 0:\n",
        "        print(\"\\nNo stations found with US filter. Trying without region filter...\")\n",
        "        selected_stations = select_stations(\n",
        "            station_inventory,\n",
        "            region=None,  # No region filter\n",
        "            max_stations=20\n",
        "        )\n",
        "    \n",
        "    print(f\"\\nSelected {len(selected_stations)} stations\")\n",
        "    \n",
        "    if len(selected_stations) > 0:\n",
        "        print(\"\\nSelected stations:\")\n",
        "        # Show available columns\n",
        "        cols_to_show = ['USAF', 'WBAN', 'STATION NAME']\n",
        "        if 'LAT' in selected_stations.columns:\n",
        "            cols_to_show.append('LAT')\n",
        "        if 'LON' in selected_stations.columns:\n",
        "            cols_to_show.append('LON')\n",
        "        if 'ELEV(M)' in selected_stations.columns:\n",
        "            cols_to_show.append('ELEV(M)')\n",
        "        if 'CTRY' in selected_stations.columns:\n",
        "            cols_to_show.append('CTRY')\n",
        "        \n",
        "        available_cols = [c for c in cols_to_show if c in selected_stations.columns]\n",
        "        if available_cols:\n",
        "            print(selected_stations[available_cols].head(10))\n",
        "        else:\n",
        "            print(selected_stations.head(10))\n",
        "    else:\n",
        "        print(\"\\nNo stations selected from inventory. Using predefined station list.\")\n",
        "        selected_stations = None\n",
        "else:\n",
        "    print(\"\\nNote: Station inventory not available. Using predefined station list.\")\n",
        "    selected_stations = None\n",
        "\n",
        "# Fallback to predefined stations if needed\n",
        "# Also check if selected stations have valid data (not just inventory summary rows)\n",
        "if selected_stations is None or len(selected_stations) == 0:\n",
        "    use_predefined = True\n",
        "else:\n",
        "    # Check if stations look valid (have proper names, not year data)\n",
        "    invalid_count = 0\n",
        "    if 'STATION NAME' in selected_stations.columns:\n",
        "        for name in selected_stations['STATION NAME'].astype(str):\n",
        "            if name.startswith('202') or len(name.strip()) < 3:\n",
        "                invalid_count += 1\n",
        "    \n",
        "    # If more than half are invalid, use predefined\n",
        "    if invalid_count > len(selected_stations) * 0.5:\n",
        "        print(f\"\\nWarning: {invalid_count}/{len(selected_stations)} stations appear invalid (inventory parsing issue).\")\n",
        "        use_predefined = True\n",
        "    else:\n",
        "        use_predefined = False\n",
        "\n",
        "if use_predefined:\n",
        "    print(\"\\nUsing predefined station list (major US cities with known good station IDs).\")\n",
        "    selected_stations = pd.DataFrame({\n",
        "        'USAF': ['725030', '722950', '725300', '722430', '722780'],\n",
        "        'WBAN': ['14732', '23174', '14734', '12960', '23183'],\n",
        "        'STATION NAME': ['NEW YORK', 'LOS ANGELES', 'CHICAGO', 'HOUSTON', 'PHOENIX'],\n",
        "        'LAT': [40.78, 34.05, 41.98, 29.98, 33.43],\n",
        "        'LON': [-73.97, -118.24, -87.90, -95.36, -112.01],\n",
        "        'ELEV(M)': [10, 71, 182, 13, 331],\n",
        "        'CTRY': ['US', 'US', 'US', 'US', 'US']\n",
        "    })\n",
        "    print(f\"Using {len(selected_stations)} predefined stations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_isd_hourly_line(line: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Parse a single line from ISD hourly data file.\n",
        "    \n",
        "    Handles both ISD-Lite and full ISD formats. Detects format based on line length.\n",
        "    \n",
        "    Full ISD format (typical line length ~400 chars):\n",
        "    - Position 0-5: USAF (6 chars)\n",
        "    - Position 7-11: WBAN (5 chars)\n",
        "    - Position 13-16: Year (4 chars)\n",
        "    - Position 17-18: Month (2 chars)\n",
        "    - Position 19-20: Day (2 chars)\n",
        "    - Position 21-22: Hour (2 chars)\n",
        "    - Temperature: Look for \"+NNNN\" or \"-NNNN\" patterns in the line\n",
        "    - Wind, pressure: Similar pattern matching\n",
        "    \n",
        "    ISD-Lite format (typical line length ~100 chars):\n",
        "    - Similar positions but shorter format\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    line : str\n",
        "        Single line from ISD file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict or None\n",
        "        Parsed observation data\n",
        "    \"\"\"\n",
        "    if len(line) < 80:  # Minimum line length\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Extract basic fields - same for both formats\n",
        "        # USAF: positions 0-5 (6 chars)\n",
        "        usaf = line[0:6].strip()\n",
        "        # WBAN: positions 7-11 (5 chars, skip position 6 which may be a dash or space)\n",
        "        wban = line[7:12].strip() if len(line) > 11 else ''\n",
        "        # Year: positions 13-16\n",
        "        year_str = line[13:17].strip() if len(line) > 16 else ''\n",
        "        # Month: positions 17-18\n",
        "        month_str = line[17:19].strip() if len(line) > 18 else ''\n",
        "        # Day: positions 19-20\n",
        "        day_str = line[19:21].strip() if len(line) > 20 else ''\n",
        "        # Hour: positions 21-22\n",
        "        hour_str = line[21:23].strip() if len(line) > 22 else ''\n",
        "        \n",
        "        # Validate and convert date/time\n",
        "        if not (year_str.isdigit() and month_str.isdigit() and day_str.isdigit() and hour_str.isdigit()):\n",
        "            return None\n",
        "        \n",
        "        year = int(year_str)\n",
        "        month = int(month_str)\n",
        "        day = int(day_str)\n",
        "        hour = int(hour_str)\n",
        "        \n",
        "        # Validate date\n",
        "        if not (1900 <= year <= 2100 and 1 <= month <= 12 and 1 <= day <= 31 and 0 <= hour <= 23):\n",
        "            return None\n",
        "        \n",
        "        # Create timestamp\n",
        "        try:\n",
        "            timestamp = datetime(year, month, day, hour)\n",
        "        except ValueError:\n",
        "            return None\n",
        "        \n",
        "        # Detect format: full ISD has longer lines with more fields\n",
        "        is_full_format = len(line) > 200\n",
        "        \n",
        "        # Parse temperature - look for patterns like +NNNN or -NNNN\n",
        "        # In full ISD, temperature can appear in multiple places, look for the air temperature field\n",
        "        temperature_2m = np.nan\n",
        "        dewpoint_2m = np.nan\n",
        "        surface_pressure = np.nan\n",
        "        wind_direction_10m = np.nan\n",
        "        wind_speed_10m = np.nan\n",
        "        \n",
        "        if is_full_format:\n",
        "            # Full ISD format - use regex to find temperature patterns\n",
        "            # Air temperature typically appears as +NNNN or -NNNN (in tenths of degrees C)\n",
        "            # Look for pattern: sign + 4 digits, preceded by certain markers\n",
        "            import re\n",
        "            \n",
        "            # Temperature: Look for +NNNN or -NNNN patterns (air temperature)\n",
        "            # In full ISD, air temp is often around position 87-92 or in ADD section\n",
        "            temp_match = re.search(r'([+-]\\d{4,5})', line[80:120])  # Search in common temp area\n",
        "            if temp_match:\n",
        "                temp_str = temp_match.group(1)\n",
        "                try:\n",
        "                    temp_val = float(temp_str) / 10.0\n",
        "                    if -100 <= temp_val <= 100:\n",
        "                        temperature_2m = temp_val\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Dew point: similar pattern\n",
        "            dew_match = re.search(r'([+-]\\d{3,4})', line[90:130])\n",
        "            if dew_match:\n",
        "                dew_str = dew_match.group(1)\n",
        "                try:\n",
        "                    dew_val = float(dew_str) / 10.0\n",
        "                    if -100 <= dew_val <= 100:\n",
        "                        dewpoint_2m = dew_val\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Sea level pressure: Look for patterns like +NNNN (in tenths of hPa)\n",
        "            # Pressure often appears around position 99-104\n",
        "            press_match = re.search(r'([+-]\\d{4,5})', line[95:115])\n",
        "            if press_match:\n",
        "                press_str = press_match.group(1)\n",
        "                try:\n",
        "                    press_val = float(press_str) / 10.0\n",
        "                    if 500 <= press_val <= 1100:\n",
        "                        surface_pressure = press_val\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Wind direction: Look for 3-digit patterns (degrees, 0-360)\n",
        "            wind_dir_match = re.search(r'\\b(\\d{3})\\b', line[60:70])\n",
        "            if wind_dir_match:\n",
        "                wind_dir_str = wind_dir_match.group(1)\n",
        "                try:\n",
        "                    wind_dir_val = float(wind_dir_str)\n",
        "                    if 0 <= wind_dir_val <= 360 and wind_dir_str != '999':\n",
        "                        wind_direction_10m = wind_dir_val\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Wind speed: Look for patterns like NNNN (in tenths of m/s)\n",
        "            wind_speed_match = re.search(r'\\b(\\d{3,4})\\b', line[65:75])\n",
        "            if wind_speed_match:\n",
        "                wind_speed_str = wind_speed_match.group(1)\n",
        "                try:\n",
        "                    wind_speed_val = float(wind_speed_str) / 10.0\n",
        "                    if 0 <= wind_speed_val <= 200 and wind_speed_str != '9999':\n",
        "                        wind_speed_10m = wind_speed_val\n",
        "                except:\n",
        "                    pass\n",
        "        else:\n",
        "            # ISD-Lite format - use fixed positions\n",
        "            # Temperature: position 24-28\n",
        "            temp_str = line[24:29].strip() if len(line) > 28 else ''\n",
        "            if temp_str and temp_str not in ['+9999', '99999', '+999']:\n",
        "                try:\n",
        "                    temperature_2m = float(temp_str) / 10.0\n",
        "                    if not (-100 <= temperature_2m <= 100):\n",
        "                        temperature_2m = np.nan\n",
        "                except:\n",
        "                    temperature_2m = np.nan\n",
        "            \n",
        "            # Dew point: position 30-33\n",
        "            dewpoint_str = line[30:34].strip() if len(line) > 33 else ''\n",
        "            if dewpoint_str and dewpoint_str not in ['+999', '9999']:\n",
        "                try:\n",
        "                    dewpoint_2m = float(dewpoint_str) / 10.0\n",
        "                    if not (-100 <= dewpoint_2m <= 100):\n",
        "                        dewpoint_2m = np.nan\n",
        "                except:\n",
        "                    dewpoint_2m = np.nan\n",
        "            \n",
        "            # Pressure: position 35-38\n",
        "            pressure_str = line[35:39].strip() if len(line) > 38 else ''\n",
        "            if pressure_str and pressure_str not in ['+999', '9999']:\n",
        "                try:\n",
        "                    surface_pressure = float(pressure_str) / 10.0\n",
        "                    if not (500 <= surface_pressure <= 1100):\n",
        "                        surface_pressure = np.nan\n",
        "                except:\n",
        "                    surface_pressure = np.nan\n",
        "            \n",
        "            # Wind direction: position 40-43\n",
        "            wind_dir_str = line[40:44].strip() if len(line) > 43 else ''\n",
        "            if wind_dir_str and wind_dir_str not in ['999', '9999']:\n",
        "                try:\n",
        "                    wind_direction_10m = float(wind_dir_str)\n",
        "                    if not (0 <= wind_direction_10m <= 360):\n",
        "                        wind_direction_10m = np.nan\n",
        "                except:\n",
        "                    wind_direction_10m = np.nan\n",
        "            \n",
        "            # Wind speed: position 46-50\n",
        "            wind_speed_str = line[46:51].strip() if len(line) > 50 else ''\n",
        "            if wind_speed_str and wind_speed_str not in ['9999', '99999']:\n",
        "                try:\n",
        "                    wind_speed_10m = float(wind_speed_str) / 10.0\n",
        "                    if not (0 <= wind_speed_10m <= 200):\n",
        "                        wind_speed_10m = np.nan\n",
        "                except:\n",
        "                    wind_speed_10m = np.nan\n",
        "        \n",
        "        # Calculate relative humidity from temperature and dew point\n",
        "        if not np.isnan(temperature_2m) and not np.isnan(dewpoint_2m):\n",
        "            try:\n",
        "                e_sat = 6.112 * np.exp(17.67 * temperature_2m / (temperature_2m + 243.5))\n",
        "                e_act = 6.112 * np.exp(17.67 * dewpoint_2m / (dewpoint_2m + 243.5))\n",
        "                relative_humidity_2m = 100.0 * (e_act / e_sat)\n",
        "                if not (0 <= relative_humidity_2m <= 100):\n",
        "                    relative_humidity_2m = np.nan\n",
        "            except:\n",
        "                relative_humidity_2m = np.nan\n",
        "        else:\n",
        "            relative_humidity_2m = np.nan\n",
        "        \n",
        "        # Only return if we have at least one valid measurement\n",
        "        if (np.isnan(temperature_2m) and np.isnan(dewpoint_2m) and \n",
        "            np.isnan(surface_pressure) and np.isnan(wind_speed_10m) and \n",
        "            np.isnan(wind_direction_10m)):\n",
        "            return None\n",
        "        \n",
        "        return {\n",
        "            'station_id': f\"{usaf}-{wban}\",\n",
        "            'timestamp': timestamp,\n",
        "            'temperature_2m': temperature_2m,\n",
        "            'dewpoint_2m': dewpoint_2m,\n",
        "            'relative_humidity_2m': relative_humidity_2m,\n",
        "            'wind_speed_10m': wind_speed_10m,\n",
        "            'wind_direction_10m': wind_direction_10m,\n",
        "            'surface_pressure': surface_pressure\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def get_available_years() -> List[int]:\n",
        "    \"\"\"\n",
        "    Check which years are available on the NOAA ISD HTTPS server.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    List[int]\n",
        "        List of available years\n",
        "    \"\"\"\n",
        "    # Try to get available years from HTTPS endpoint\n",
        "    # NCEI typically has data from 1900s to present\n",
        "    current_year = datetime.now().year\n",
        "    # For now, return a reasonable range - we'll verify existence when downloading\n",
        "    # Common years with data: 2000 to current\n",
        "    return list(range(2000, current_year + 1))\n",
        "\n",
        "def download_isd_station_data(\n",
        "    usaf: str,\n",
        "    wban: str,\n",
        "    year: int,\n",
        "    output_dir: Optional[Path] = None,\n",
        "    max_retries: int = 3\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Download ISD hourly data for a specific station and year using HTTPS.\n",
        "    \n",
        "    Uses NCEI HTTPS endpoint with retry logic and exponential backoff.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    usaf : str\n",
        "        USAF station identifier\n",
        "    wban : str\n",
        "        WBAN station identifier\n",
        "    year : int\n",
        "        Year to download\n",
        "    output_dir : Path, optional\n",
        "        Directory to save raw files\n",
        "    max_retries : int\n",
        "        Maximum number of retry attempts for failed downloads\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame or None\n",
        "        Parsed hourly observations\n",
        "    \"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = RAW_DATA_DIR / \"noaa_isd_raw\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    station_id = f\"{usaf}-{wban}\"\n",
        "    filename = f\"{usaf}-{wban}-{year}.gz\"\n",
        "    local_file = output_dir / filename\n",
        "    \n",
        "    # NCEI HTTPS endpoint for global-hourly data\n",
        "    base_url = \"https://www.ncei.noaa.gov/data/global-hourly/access\"\n",
        "    url = f\"{base_url}/{year}/{filename}\"\n",
        "    \n",
        "    # Download with retry logic\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Use requests with streaming and timeout\n",
        "            response = requests.get(\n",
        "                url,\n",
        "                stream=True,\n",
        "                timeout=(30, 300),  # (connect timeout, read timeout) - 5 min for large files\n",
        "                headers={'User-Agent': 'Mozilla/5.0 (compatible; ISD-Downloader/1.0)'}\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # Download to temporary file first, then rename on success\n",
        "            temp_file = local_file.with_suffix('.tmp')\n",
        "            total_size = 0\n",
        "            expected_size = int(response.headers.get('Content-Length', 0))\n",
        "            \n",
        "            with open(temp_file, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        total_size += len(chunk)\n",
        "            \n",
        "            # Verify file size\n",
        "            if expected_size > 0 and total_size != expected_size:\n",
        "                print(f\"  Warning: Downloaded size ({total_size}) != expected ({expected_size})\")\n",
        "                # Still try to use it if it's reasonable (> 1KB)\n",
        "                if total_size < 1024:\n",
        "                    temp_file.unlink()\n",
        "                    raise ValueError(f\"File too small: {total_size} bytes (likely error page)\")\n",
        "            \n",
        "            # Verify file is not empty\n",
        "            if total_size == 0:\n",
        "                temp_file.unlink()\n",
        "                raise ValueError(\"Downloaded file is empty\")\n",
        "            \n",
        "            # Rename temp file to final name\n",
        "            if temp_file.exists():\n",
        "                if local_file.exists():\n",
        "                    local_file.unlink()\n",
        "                temp_file.rename(local_file)\n",
        "            \n",
        "            print(f\"  Downloaded {filename} ({total_size:,} bytes)\")\n",
        "            break  # Success, exit retry loop\n",
        "            \n",
        "        except requests.exceptions.Timeout as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n",
        "                print(f\"  Timeout (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"  Download timeout after {max_retries} attempts: {e}\")\n",
        "                if local_file.exists():\n",
        "                    local_file.unlink()\n",
        "                return None\n",
        "                \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"  Network error (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            else:\n",
        "                # Check if it's a 404 (file doesn't exist) vs other error\n",
        "                if hasattr(e, 'response') and e.response is not None:\n",
        "                    if e.response.status_code == 404:\n",
        "                        # File doesn't exist - this is \"no data\", not a network error\n",
        "                        return None\n",
        "                print(f\"  Download failed after {max_retries} attempts: {e}\")\n",
        "                if local_file.exists():\n",
        "                    local_file.unlink()\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Unexpected error during download: {e}\")\n",
        "            if local_file.exists():\n",
        "                local_file.unlink()\n",
        "            return None\n",
        "    \n",
        "    # Verify file exists and has reasonable size before parsing\n",
        "    if not local_file.exists():\n",
        "        return None\n",
        "    \n",
        "    file_size = local_file.stat().st_size\n",
        "    if file_size < 1024:  # Less than 1KB is suspicious\n",
        "        print(f\"  Warning: File size is very small ({file_size} bytes), may be an error page\")\n",
        "        # Still try to parse it - debugging will show what's in it\n",
        "        \n",
        "    # Parse file (handle both .gz and uncompressed)\n",
        "    observations = []\n",
        "    sample_lines = []\n",
        "    try:\n",
        "        if filename.endswith('.gz'):\n",
        "            with gzip.open(local_file, 'rt', encoding='latin-1', errors='ignore') as f:\n",
        "                line_count = 0\n",
        "                for line in f:\n",
        "                    line_count += 1\n",
        "                    # Save first few lines for debugging\n",
        "                    if line_count <= 3:\n",
        "                        sample_lines.append(line.rstrip())\n",
        "                    obs = parse_isd_hourly_line(line)\n",
        "                    if obs:\n",
        "                        observations.append(obs)\n",
        "        else:\n",
        "            with open(local_file, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "                line_count = 0\n",
        "                for line in f:\n",
        "                    line_count += 1\n",
        "                    # Save first few lines for debugging\n",
        "                    if line_count <= 3:\n",
        "                        sample_lines.append(line.rstrip())\n",
        "                    obs = parse_isd_hourly_line(line)\n",
        "                    if obs:\n",
        "                        observations.append(obs)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error parsing file: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "    \n",
        "    # Debug: Show sample lines if no observations found\n",
        "    if len(observations) == 0:\n",
        "        print(f\"  No valid observations found after parsing\")\n",
        "        if sample_lines:\n",
        "            print(f\"  Sample lines from file (first 3, length {len(sample_lines[0]) if sample_lines else 0}):\")\n",
        "            for i, line in enumerate(sample_lines):\n",
        "                print(f\"    Line {i+1}: {repr(line[:150])}\")\n",
        "        else:\n",
        "            print(f\"  File appears empty or unreadable\")\n",
        "        return None\n",
        "    \n",
        "    df = pd.DataFrame(observations)\n",
        "    df['usaf'] = usaf\n",
        "    df['wban'] = wban\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Check available years\n",
        "print(\"=\" * 60)\n",
        "print(\"Checking Available Years on NOAA ISD Server\")\n",
        "print(\"=\" * 60)\n",
        "available_years = get_available_years()\n",
        "print(f\"Available years: {available_years[-10:]}\")  # Show last 10 years\n",
        "\n",
        "# Filter to only download years that exist and are in our range\n",
        "years_to_download = [y for y in range(START_YEAR, END_YEAR + 1) if y in available_years]\n",
        "print(f\"Years to download: {years_to_download}\")\n",
        "\n",
        "if not years_to_download:\n",
        "    print(f\"\\nWarning: No data available for years {START_YEAR}-{END_YEAR}\")\n",
        "    print(f\"Available years: {available_years}\")\n",
        "    print(\"Adjusting date range to use available years...\")\n",
        "    if available_years:\n",
        "        years_to_download = available_years[-2:]  # Use last 2 available years\n",
        "        START_YEAR = min(years_to_download)\n",
        "        END_YEAR = max(years_to_download)\n",
        "        START_DATE = datetime(START_YEAR, 1, 1)\n",
        "        END_DATE = datetime(END_YEAR, 12, 31)\n",
        "        print(f\"Using years: {years_to_download}\")\n",
        "\n",
        "# Download data for selected stations\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Downloading NOAA ISD Hourly Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for idx, station in tqdm(selected_stations.iterrows(), total=len(selected_stations), desc=\"Stations\"):\n",
        "    usaf = str(station.get('USAF', '')).zfill(6)\n",
        "    wban = str(station.get('WBAN', '')).zfill(5)\n",
        "    station_name = station.get('STATION NAME', f\"{usaf}-{wban}\")\n",
        "    \n",
        "    # Skip invalid stations\n",
        "    if not usaf.isdigit() or len(usaf) < 5:\n",
        "        print(f\"\\nSkipping invalid station: {station_name} (USAF: {usaf})\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nStation: {station_name} ({usaf}-{wban})\")\n",
        "    \n",
        "    station_data_found = False\n",
        "    for year in years_to_download:\n",
        "        try:\n",
        "            df_year = download_isd_station_data(usaf, wban, year)\n",
        "            if df_year is not None and len(df_year) > 0:\n",
        "                # Filter by date range\n",
        "                df_year = df_year[\n",
        "                    (df_year['timestamp'] >= START_DATE) & \n",
        "                    (df_year['timestamp'] <= END_DATE)\n",
        "                ]\n",
        "                if len(df_year) > 0:\n",
        "                    all_data.append(df_year)\n",
        "                    print(f\"  {year}: {len(df_year)} observations\")\n",
        "                    station_data_found = True\n",
        "        except Exception as e:\n",
        "            print(f\"  Error for {year}: {e}\")\n",
        "            continue\n",
        "        time.sleep(0.5)  # Be polite to server\n",
        "    \n",
        "    if not station_data_found:\n",
        "        print(f\"  No data found for this station\")\n",
        "\n",
        "# Combine all data\n",
        "if all_data:\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"\\nTotal observations: {len(combined_df)}\")\n",
        "    print(f\"Date range: {combined_df['timestamp'].min()} to {combined_df['timestamp'].max()}\")\n",
        "    print(f\"Stations: {combined_df['station_id'].nunique()}\")\n",
        "else:\n",
        "    print(\"\\nNo data downloaded. Check station IDs and FTP connection.\")\n",
        "    combined_df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Validation and Quality Checks\n",
        "\n",
        "Check data quality, missing values, and temporal coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Data Quality Summary\n",
            "============================================================\n",
            "\n",
            "Total observations: 9,612,898\n",
            "Unique stations: 1090\n",
            "Date range: 2022-01-01 00:00:00 to 2022-12-31 23:59:00\n",
            "\n",
            "Missing data percentage:\n",
            "temperature_2m           2.34\n",
            "dewpoint_2m              3.34\n",
            "wind_direction_10m      11.16\n",
            "wind_speed_10m           6.06\n",
            "surface_pressure        55.88\n",
            "relative_humidity_2m     3.37\n",
            "dtype: float64\n",
            "\n",
            "Observations per station:\n",
            "station_id\n",
            "6235099999    40458\n",
            "6242099999    40327\n",
            "6060099999    38197\n",
            "6030099999    34401\n",
            "6110099999    27543\n",
            "3969099999    26082\n",
            "6201099999    26078\n",
            "3962099999    26077\n",
            "6203099999    26073\n",
            "6211099999    26071\n",
            "dtype: int64\n",
            "\n",
            "Temporal coverage (observations per day):\n",
            "Mean: 26336.7\n",
            "Min: 19398\n",
            "Max: 29683\n",
            "\n",
            "Variable statistics:\n",
            "       temperature_2m  relative_humidity_2m  wind_speed_10m  \\\n",
            "count    9.388317e+06          9.288997e+06    9.030249e+06   \n",
            "mean     7.164884e+00          7.962010e+01    4.843662e+00   \n",
            "std      8.497164e+00          1.566240e+01    3.685883e+00   \n",
            "min     -5.990000e+01          4.715105e-04    0.000000e+00   \n",
            "25%      2.000000e+00          7.048140e+01    2.100000e+00   \n",
            "50%      8.000000e+00          8.249524e+01    4.000000e+00   \n",
            "75%      1.300000e+01          9.274416e+01    6.400000e+00   \n",
            "max      4.700000e+01          1.000000e+02    4.940000e+01   \n",
            "\n",
            "       wind_direction_10m  surface_pressure  \n",
            "count        8.539737e+06      4.241191e+06  \n",
            "mean         1.993835e+02      1.012519e+03  \n",
            "std          9.269556e+01      1.255302e+01  \n",
            "min          1.000000e+00      9.021000e+02  \n",
            "25%          1.300000e+02      1.005300e+03  \n",
            "50%          2.100000e+02      1.013700e+03  \n",
            "75%          2.700000e+02      1.021100e+03  \n",
            "max          3.600000e+02      1.059500e+03  \n"
          ]
        }
      ],
      "source": [
        "if combined_df is not None and len(combined_df) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Data Quality Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"\\nTotal observations: {len(combined_df):,}\")\n",
        "    print(f\"Unique stations: {combined_df['station_id'].nunique()}\")\n",
        "    print(f\"Date range: {combined_df['timestamp'].min()} to {combined_df['timestamp'].max()}\")\n",
        "    \n",
        "    # Missing data analysis\n",
        "    print(\"\\nMissing data percentage:\")\n",
        "    missing_pct = (combined_df.isnull().sum() / len(combined_df) * 100).round(2)\n",
        "    print(missing_pct[missing_pct > 0])\n",
        "    \n",
        "    # Data coverage by station\n",
        "    print(\"\\nObservations per station:\")\n",
        "    station_counts = combined_df.groupby('station_id').size().sort_values(ascending=False)\n",
        "    print(station_counts.head(10))\n",
        "    \n",
        "    # Temporal coverage\n",
        "    print(\"\\nTemporal coverage (observations per day):\")\n",
        "    daily_counts = combined_df.groupby(combined_df['timestamp'].dt.date).size()\n",
        "    print(f\"Mean: {daily_counts.mean():.1f}\")\n",
        "    print(f\"Min: {daily_counts.min()}\")\n",
        "    print(f\"Max: {daily_counts.max()}\")\n",
        "    \n",
        "    # Variable statistics\n",
        "    print(\"\\nVariable statistics:\")\n",
        "    numeric_cols = ['temperature_2m', 'relative_humidity_2m', 'wind_speed_10m', \n",
        "                    'wind_direction_10m', 'surface_pressure']\n",
        "    print(combined_df[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"No data available for validation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Processed Data\n",
        "\n",
        "Save the cleaned data to parquet format for efficient storage and loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\noaa_isd_raw_data.parquet\n",
            "File size: 78.41 MB\n",
            "Sample CSV saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\noaa_isd_raw_data_sample.csv\n",
            "Station metadata saved to: C:\\Users\\Kata\\Desktop\\earth-sgnn\\data\\raw\\noaa_isd_station_metadata.csv\n",
            "\n",
            "============================================================\n",
            "Data acquisition complete!\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "1. Review data quality in preprocessing notebook\n",
            "2. Handle missing values and outliers\n",
            "3. Create graph structure from station coordinates\n"
          ]
        }
      ],
      "source": [
        "if combined_df is not None and len(combined_df) > 0:\n",
        "    # Save raw data\n",
        "    raw_data_file = RAW_DATA_DIR / \"noaa_isd_raw_data.parquet\"\n",
        "    combined_df.to_parquet(raw_data_file, index=False)\n",
        "    print(f\"Raw data saved to: {raw_data_file}\")\n",
        "    print(f\"File size: {raw_data_file.stat().st_size / (1024*1024):.2f} MB\")\n",
        "    \n",
        "    # Also save as CSV for easy inspection (sample)\n",
        "    csv_file = RAW_DATA_DIR / \"noaa_isd_raw_data_sample.csv\"\n",
        "    combined_df.head(10000).to_csv(csv_file, index=False)\n",
        "    print(f\"Sample CSV saved to: {csv_file}\")\n",
        "    \n",
        "    # Save station metadata\n",
        "    if selected_stations is not None:\n",
        "        metadata_file = RAW_DATA_DIR / \"noaa_isd_station_metadata.csv\"\n",
        "        selected_stations.to_csv(metadata_file, index=False)\n",
        "        print(f\"Station metadata saved to: {metadata_file}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Data acquisition complete!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(\"1. Review data quality in preprocessing notebook\")\n",
        "    print(\"2. Handle missing values and outliers\")\n",
        "    print(\"3. Create graph structure from station coordinates\")\n",
        "else:\n",
        "    print(\"No data to save. Please check:\")\n",
        "    print(\"1. Station IDs are correct\")\n",
        "    print(\"2. FTP connection is working\")\n",
        "    print(\"3. Data files exist for selected years\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-earth-sgnn (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
